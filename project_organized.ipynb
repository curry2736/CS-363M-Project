{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "  if(x['year'] <= 1993): #rounds before 1993 did not have 32 rounds per pick. We should standardize to today's standard\n",
    "    x['rnd'] = 1 + int(x['pick'] / 32)\n",
    "  return x\n",
    "\n",
    "data = data.apply(func=transform, axis=1, result_type='broadcast')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_as_ints = data['rnd']\n",
    "data.loc[rounds_as_ints <= 3, 'rnd'] = '1-3'\n",
    "data.loc[rounds_as_ints > 3 and data['rnd'] <= 6 , 'rnd'] = '4-6'\n",
    "data.loc[rounds_as_ints > 6, 'rnd'] = '>7'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['rnd']\n",
    "\n",
    "#pick directly correlates with round. keeping it as a feature would be data leakage\n",
    "features = data.drop(['pick'], axis=1)\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cmps.loc[null_cmps['pos'] == 'QB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[['first4av', 'rnd']]\n",
    "plt.scatter(x=features['first4av'], y=features['rnd'], marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use counter to get frequency of each label\n",
    "frequency = collections.Counter(labels)\n",
    "\n",
    "# printing the frequency to view any class imbalances between the rounds\n",
    "print(dict(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "# params = {\"max_depth\": [5,10,15,20], \"min_samples_leaf\": [5,10,15,20]}\n",
    "# grid_search = GridSearchCV(clf, params, cv=5, scoring='accuracy') #inner loop\n",
    "# replace clf with grid_search if you want to test parameters\n",
    "nested_score = cross_val_score(clf, features, labels, cv=5) #outer loop\n",
    "print(\"Accuracy:\", nested_score.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, features, labels, cv=10)                                       \n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "#Alternative (cross_val_predict instead of cross_val_score) to analyze the results in more detail:\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "clf = GaussianNB()\n",
    "predicts = cross_val_predict(clf, features, labels, cv=10) \n",
    "print(\"Predictions:\", predicts) \n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels, predicts))\n",
    "print(\"Report:\\n\", classification_report(labels, predicts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
