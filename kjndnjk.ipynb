{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_COLUMNS = [\n",
    "    'st', 'carav', 'g', \n",
    "    'cmp', 'pass_att', 'pass_yds', 'pass_td', 'pass_int',\n",
    "    'rush_att', 'rush_yds', 'rush_tds',\n",
    "    'rec', 'rec_yds', 'rec_tds', 'tkl',\n",
    "    'def_int'\n",
    "]\n",
    "\n",
    "POSITION_DEPENDENT_FEATURES = [\n",
    "    'cmp', 'pass_att', 'pass_yds', 'pass_td', 'pass_int', 'rush_att', 'rush_yds', 'rush_tds', 'rec', 'rec_yds', 'rec_tds', 'tkl', 'def_int', 'sk', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and preview dataset\n",
    "data = pd.read_csv(\"nfl_draft.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of null entries in each feature\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many features have null values. We suspect that the reason is that some\n",
    "#positions do not do things that other positions do. For example, a linebacker\n",
    "#will likely never throw a football, so they will have null for cmp\n",
    "#(completions)\n",
    "\n",
    "#let's verify and check how many QB's have null completions vs total amount of\n",
    "#null completions\n",
    "null_cmps = data[data['cmp'].isnull()]\n",
    "null_cmps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cmps.loc[null_cmps['pos'] == 'QB']\n",
    "\n",
    "# as we can see, very few QB positions have null values for cmp while many non\n",
    "# QB positions do. Because of this, we think it is best to impute a value of 0 \n",
    "# for the position-dependent features completions, rush attempts, touchdowns, \n",
    "# etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[POSITION_DEPENDENT_FEATURES] = data[POSITION_DEPENDENT_FEATURES].fillna(0)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['position_standard'].value_counts()\n",
    "#LS is a specialized version of C, so we can combine the two into the same position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['position_standard'] = data['position_standard'].replace(['LS'], 'C')\n",
    "data['position_standard'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "  if(x['year'] <= 1993): #rounds before 1993 did not have 32 rounds per pick. We should standardize to today's standard\n",
    "    x['rnd'] = 1 + int(x['pick'] / 32)\n",
    "  return x\n",
    "\n",
    "data = data.apply(func=transform, axis=1, result_type='broadcast')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_as_ints = data['rnd']\n",
    "data.loc[rounds_as_ints <= 3, 'rnd'] = '1-3'\n",
    "data.loc[rounds_as_ints > 3 and data['rnd'] <= 6 , 'rnd'] = '4-6'\n",
    "data.loc[rounds_as_ints > 6, 'rnd'] = '>7'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['rnd']\n",
    "\n",
    "#pick directly correlates with round. keeping it as a feature would be data leakage\n",
    "features = data.drop(['pick'], axis=1)\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['rnd'].corr(data['drav']))\n",
    "print(data['rnd'].corr(data['carav']))\n",
    "print(data['drav'].corr(data['carav']))\n",
    "\n",
    "#drav and carav basically describe the same thing (career average and draft\n",
    "#average)\n",
    "#this can be seen from their correlation value\n",
    "#we decide to drop drav because corr with rnd lower than carav and they are similar\n",
    "data[['st', 'g', 'cmp', 'pass_att',\n",
    "       'pass_yds', 'pass_td', 'pass_int', 'rush_att', 'rush_yds', 'rush_tds',\n",
    "       'rec', 'rec_yds', 'rec_tds', 'tkl', 'def_int', 'sk']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dropping ap1 (Deals with all pro designation), pb (Deals with Pro Bowler\n",
    "designation), hof (hall of fame designation)\n",
    "This is future data (received after they were drafted and cannot be used in\n",
    "prediction)\n",
    "'''\n",
    "features = data.drop(['ap1', 'pb', 'to'], axis=1)\n",
    "\n",
    "#unneeded  things like name/player id, team, etc. Does not help with our prediction\n",
    "features.drop(['column_a','player_id','tm', 'hof', 'player', 'pos'], axis=1, inplace=True)\n",
    "\n",
    "#drop things with too many nulls that cannot be imputed\n",
    "features.drop(['college_univ'], axis=1, inplace=True)\n",
    "\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cmps.loc[null_cmps['pos'] == 'QB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[['first4av', 'rnd']]\n",
    "plt.scatter(x=features['first4av'], y=features['rnd'], marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(features['position_standard'])\n",
    "features = features.drop('position_standard', axis=1)\n",
    "features = features.join(one_hot)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use counter to get frequency of each label\n",
    "frequency = collections.Counter(labels)\n",
    "\n",
    "# printing the frequency to view any class imbalances between the rounds\n",
    "print(dict(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "# params = {\"max_depth\": [5,10,15,20], \"min_samples_leaf\": [5,10,15,20]}\n",
    "# grid_search = GridSearchCV(clf, params, cv=5, scoring='accuracy') #inner loop\n",
    "# replace clf with grid_search if you want to test parameters\n",
    "nested_score = cross_val_score(clf, features, labels, cv=5) #outer loop\n",
    "print(\"Accuracy:\", nested_score.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, features, labels, cv=10)                                       \n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "#Alternative (cross_val_predict instead of cross_val_score) to analyze the results in more detail:\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "clf = GaussianNB()\n",
    "predicts = cross_val_predict(clf, features, labels, cv=10) \n",
    "print(\"Predictions:\", predicts) \n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels, predicts))\n",
    "print(\"Report:\\n\", classification_report(labels, predicts))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
